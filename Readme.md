# Evident Video Fact Checker

A local fact-checking pipeline for video transcripts. Extracts claims, searches for evidence, verifies each claim with LLM reasoning, and generates a detailed report.

Everything runs locally — no cloud APIs, no data leaves your machine.

- **Transcript ingestion** - Upload a file or paste a YouTube URL; auto-fetches captions or transcribes locally with Whisper
- **Claim extraction** - Scan transcript to gather a list of claims made in the video
- **Evidence retrieval** - Search for corroborating evidence to claims gathered. 6-tier quality system prioritizes scholarly sources over forums/blogs
- **Claim verification** - LLM reasoning with citations, confidence scoring, and rhetorical analysis
- **Report generation** - Detailed readout of the summary of all claims researched

## Quick Start

```bash
# One-command setup (installs deps, detects hardware, downloads models, starts services)
python setup.py

# Fact-check a YouTube video
python -m app.main --url "https://www.youtube.com/watch?v=VIDEO_ID"

# Or start the web UI
python -m app.web.server
```

## Setup

### Prerequisites

| Requirement | Purpose | Install |
|---|---|---|
| **Python 3.11+** | Runtime | [python.org](https://www.python.org/downloads/) |
| **Ollama** | Local LLM inference | [ollama.com](https://ollama.com/download) |
| **Docker** | Runs SearXNG search engine | [docker.com](https://www.docker.com/products/docker-desktop) |
| FFmpeg *(optional)* | YouTube Whisper fallback | `winget install Gyan.FFmpeg` |

### Setup Wizard

The setup wizard handles everything after the prerequisites are installed:

```bash
python setup.py
```

It will:
1. Install Python packages from `Requirements.txt`
2. Verify Ollama is running and start it if needed
3. Detect your GPU and RAM, recommend models
4. Download Ollama models (qwen3, gemma3)
5. Start SearXNG + Redis via Docker
6. Generate `config.yaml` and `.env`
7. Run a smoke test to verify everything works

### Hardware Recommendations

| GPU VRAM | System RAM | Models |
|---|---|---|
| 24 GB+ | 32 GB+ | qwen3:8b, qwen3:30b, gemma3:27b |
| 12–16 GB | 32 GB+ | qwen3:8b, qwen3:14b, gemma3:12b |
| None (CPU) | 32 GB+ | qwen3:8b, llama3:8b |
| None (CPU) | 16 GB | phi3:mini |

---

## Web UI

```bash
python -m app.web.server
```

Open **http://localhost:8000** in your browser.

- Paste a YouTube URL or upload a transcript file
- Watch real-time progress with per-stage counters (claims, sources, snippets)
- Optionally review and select extracted claims before verification
- View the rendered report with verdict badges and download artifacts
- Browse past runs with video titles, channels, and verdict summaries

## CLI

### YouTube URL

```bash
python -m app.main --url "https://www.youtube.com/watch?v=VIDEO_ID"
```

Auto-fetches captions when available, or transcribes locally with Whisper as fallback. Channel name is inferred from YouTube metadata.

### Transcript File

```bash
python -m app.main --infile "inbox/transcript.txt" --channel "Channel Name"
```

Place transcript files in `inbox/`. If `--infile` is omitted, the newest file in `inbox/` is used.

### CLI Flags

| Flag | Description |
|---|---|
| `--url <youtube-url>` | YouTube video URL |
| `--infile <path>` | Path to transcript file |
| `--channel <name>` | Channel/creator name (inferred if omitted) |
| `--review` | Interactive claim review before verification |
| `--verbose` | Show DEBUG output |
| `--quiet` | Errors and warnings only |

`--url` and `--infile` are mutually exclusive.

---

## Service Management

```bash
python setup.py start      # Start Ollama + SearXNG
python setup.py stop        # Stop SearXNG containers
python setup.py status      # Check service health
python setup.py --check     # Full validation + smoke test
```

If you have `make` installed, these also work:

```bash
make start                  # Same as above
make stop
make status
make check
```

---

## Configuration

### config.yaml

Primary configuration — generated by `python setup.py` or edit manually:

```yaml
ollama:
  base_url: "http://localhost:11434"
  model_extract: "qwen3:8b"
  model_verify: "qwen3:30b"
  model_write: "gemma3:27b"

searx:
  base_url: "http://localhost:8888"

budgets:
  max_claims: 25
  max_sources_per_claim: 5
  max_fetches_per_run: 80
```

### Environment Variables (.env)

Override `config.yaml` settings via environment variables:

```bash
EVIDENT_OLLAMA_BASE_URL=http://localhost:11434
EVIDENT_SEARXNG_BASE_URL=http://localhost:8888
EVIDENT_MODEL_EXTRACT=qwen3:8b
EVIDENT_MODEL_VERIFY=qwen3:30b
EVIDENT_MODEL_WRITE=gemma3:27b
```

---

## Pipeline

Each run executes these stages in order:

1. **Prepare Transcript** — Fetch from YouTube (or accept upload) and normalize to JSON segments
2. **Extract Claims** — LLM identifies checkable claims (with overlapping chunks) and consolidates duplicates into narrative groups
3. **Review Claims** *(optional)* — Select which claims to verify
4. **Gather Evidence** — LLM-generated search queries, SearXNG search, URL fetch, snippet extraction
5. **Check Claims** — LLM evaluates each claim against evidence (parallel verification)
6. **Fact-Check Summary** — Aggregate verdicts, generate report with narrative group analysis

Token usage (prompt and completion tokens) is tracked per-stage and stored in `run.json`.

### Verdict Ratings

| Rating | Meaning |
|---|---|
| VERIFIED | Confirmed by strong evidence |
| LIKELY TRUE | Supported but not fully confirmed |
| INSUFFICIENT EVIDENCE | Not enough quality sources found |
| CONFLICTING EVIDENCE | Credible sources disagree |
| LIKELY FALSE | Evidence suggests the claim is wrong |
| FALSE | Clearly contradicted by strong evidence |

### Source Quality Tiers

| Tier | Examples |
|---|---|
| 1 — Scholarly | Nature, Science, NEJM, Lancet |
| 2 — Academic | .edu, .ac.uk institutions |
| 3 — Government | .gov, WHO, UN, CDC |
| 4 — Research orgs | Pew, Brookings, RAND |
| 5 — Major news | Reuters, AP, BBC, NYT |
| 6 — Other | Everything else |

## Output

Each run creates a timestamped directory:

```
runs/YYYYMMDD_HHMMSS__channel__video_title/
├── 00_transcript.raw.txt     Original input
├── 01_transcript.json        Normalized segments
├── 02_claims.json            Extracted claims
├── 03_sources.json           Retrieved evidence
├── 04_snippets.json          Evidence snippets
├── 05_verdicts.json          Verification results
├── 06_scorecard.md           Verdict counts and source tiers
├── 07_summary.md             Full fact-check report
├── run.json                  Run metadata (config, counts, token usage, timings)
└── run.log                   Execution log
```

## Project Structure

```
evident-video-fact-checker/
├── app/
│   ├── main.py               CLI entry point
│   ├── pipeline/              Processing stages
│   ├── schemas/               Pydantic models (Claim, Verdict, ClaimGroup)
│   ├── store/                 Persistent storage modules
│   ├── tools/                 Utilities (ollama, searx, fetch, youtube)
│   └── web/                   Web UI (FastAPI + HTMX + Jinja2)
├── setup.py                   Setup wizard + service management
├── docker-compose.searxng.yml SearXNG + Redis (Docker)
├── config.yaml                Application config
├── Requirements.txt           Python dependencies
└── Makefile                   Make shortcuts
```

## Live Demo

Try the app without any setup — a static clickable demo walks through the full pipeline using real data from past fact-check runs.

**[Launch Demo](demo/index.html)**

The demo includes:
- **Landing page** with pre-filled YouTube URL and file upload modes
- **Click-through pipeline** — step through all 6 stages with animated counters, progress bars, and a live log stream
- **Claim review** — browse 23 extracted claims with type badges and transcript quotes
- **Full reports** for 3 real runs (JRE Clips, Tim Pool, Brian Tyler Cohen) with verdicts, citations, and narrative group analysis
- **Run history** — browse past fact-check results with verdict summaries

No server, no dependencies — just open `demo/index.html` in a browser, or serve it:

```bash
python -m http.server 9000 -d demo
```

## Documentation

- [DOCKER.md](DOCKER.md) — Full Docker deployment (advanced)
- [MIGRATION.md](MIGRATION.md) — Migration from legacy setup
- [CLAUDE.md](CLAUDE.md) — Project context for AI assistants
